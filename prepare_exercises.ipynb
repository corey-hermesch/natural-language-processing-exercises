{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a630b1b-5aad-486c-a192-cb7e08074213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire as a\n",
    "import prepare as p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a46f2c-e9c3-414c-85a3-064306794725",
   "metadata": {},
   "source": [
    "# Prepare Exercises\n",
    "\n",
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "    Lowercase everything\n",
    "    Normalize unicode characters\n",
    "    Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n",
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string.\n",
    "\n",
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n",
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.\n",
    "\n",
    "5. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "    This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90e3242-ab2b-4400-bf25-2171558da68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached file found and read\n"
     ]
    }
   ],
   "source": [
    "# get some text to play with\n",
    "news_articles = a.get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6afde05b-540c-4a24-9f0d-b1383aeb5a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Benchmark indices Sensex and Nifty ended at record closing highs on Wednesday. Sensex ended 195 points higher at 63,523 while the Nifty ended at 18,856.85, up 40 points. The gains were led by stocks like HDFC, Reliance Industries and TCS. During the intraday trade, Sensex rose to its fresh record high level of 63,588. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text1 = news_articles[0]['content']\n",
    "test_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bff38e-7aaa-4634-9fac-27f80fb17eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text2 = 'I had a bologna sandwich for lunch. It was super-delicious! I do wish it had cheesey cheese and maybe some mustard, mayo, relish, tomatoes, tomato, salt, pepper, butter, with a side of Hostess cakes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d92874-88a3-4d44-8a77-1d9a1c48e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benchmark indices sensex and nifty ended at record closing highs on wednesday. sensex ended 195 points higher at 63,523 while the nifty ended at 18,856.85, up 40 points. the gains were led by stocks like hdfc, reliance industries and tcs. during the intraday trade, sensex rose to its fresh record high level of 63,588. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make all text lowercase\n",
    "test = test_text1.lower()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1447015-791d-42b6-bdb6-2c2337ef2918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benchmark indices sensex and nifty ended at record closing highs on wednesday. sensex ended 195 points higher at 63,523 while the nifty ended at 18,856.85, up 40 points. the gains were led by stocks like hdfc, reliance industries and tcs. during the intraday trade, sensex rose to its fresh record high level of 63,588. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure text is \"normalized\" (remove inconsistincies in unicode character encoding)\n",
    "# AND make all characters ASCII characters, ignoring any errors in conversion (i.e. drop those chars)\n",
    "test = unicodedata.normalize('NFKD', test).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eba853d-c24f-422f-a915-545b33afb5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benchmark indices sensex and nifty ended at record closing highs on wednesday sensex ended 195 points higher at 63523 while the nifty ended at 1885685 up 40 points the gains were led by stocks like hdfc reliance industries and tcs during the intraday trade sensex rose to its fresh record high level of 63588 '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop anything that is not a letter, number, whitespace, or a single quote\n",
    "test = re.sub(r\"[^a-z0-9\\s']\", '', test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab083864-be16-4499-9322-1e8c938ea361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(article):\n",
    "    \"\"\"\n",
    "    This function will perform basic text cleaning on the given article.\n",
    "    \n",
    "    Args:\n",
    "        article (str): The text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned article.\n",
    "\n",
    "    The function applies the following steps to clean the text:\n",
    "    1. Converts the article to lowercase.\n",
    "    2. Normalizes the text by removing inconsistencies in unicode character encoding.\n",
    "    3. Converts all characters to ASCII characters, dropping any characters that cannot be converted.\n",
    "    4. Removes any characters that are not letters, numbers, whitespace, or a single quote.\n",
    "\n",
    "    Example:\n",
    "        article = \"Hello, World! This is an example article.\"\n",
    "        cleaned_article = basic_clean(article)\n",
    "        print(cleaned_article)\n",
    "        # Output: \"hello world this is an example article\"\n",
    "    \"\"\"\n",
    "    # make all text lower case\n",
    "    article = article.lower()\n",
    "    # ensure text is \"normalized\" (remove inconsistincies in unicode character encoding)\n",
    "    # AND make all characters ASCII characters, ignoring any errors in conversion (i.e. drop those chars)\n",
    "    article = unicodedata.normalize('NFKD', article).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # making hyphenated words into two words separated by a space, using positive look behind and pos look ahead\n",
    "    article = re.sub(r'(?<=\\w)-(?=\\w)', ' ', article)\n",
    "    # drop anything that is not a letter, number, whitespace, or a single quote\n",
    "    article = re.sub(r\"[^a-z0-9\\s']\", '', article)\n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf71e65e-5a1c-4c05-b0f5-468bf901c36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'super delicious'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# found this on chat gpt. it said it is using \"positive look behind and positive look ahead\"\n",
    "test = 'super-delicious'\n",
    "re.sub(r\"(?<=\\w)-(?=\\w)\", ' ', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43392b31-b52b-4f2b-b1ff-c596ec3ca6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Benchmark indices Sensex and Nifty ended at record closing highs on Wednesday. Sensex ended 195 points higher at 63,523 while the Nifty ended at 18,856.85, up 40 points. The gains were led by stocks like HDFC, Reliance Industries and TCS. During the intraday trade, Sensex rose to its fresh record high level of 63,588. ',\n",
       " 'I had a bologna sandwich for lunch. It was super-delicious! I do wish it had cheesey cheese and maybe some mustard, mayo, relish, tomatoes, tomato, salt, pepper, butter, with a side of Hostess cakes.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_text1, test_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16ecfbd-6e49-46ba-836a-e41a1399d3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benchmark indices sensex and nifty ended at record closing highs on wednesday sensex ended 195 points higher at 63523 while the nifty ended at 1885685 up 40 points the gains were led by stocks like hdfc reliance industries and tcs during the intraday trade sensex rose to its fresh record high level of 63588 ',\n",
       " 'i had a bologna sandwich for lunch it was super delicious i do wish it had cheesey cheese and maybe some mustard mayo relish tomatoes tomato salt pepper butter with a side of hostess cakes')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic1 = basic_clean(test_text1)\n",
    "basic2 = basic_clean(test_text2)\n",
    "(basic1, basic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "555ad7aa-c0f7-427a-8217-d06fc3e910ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benchmark indices sensex and nifty ended at record closing highs on wednesday sensex ended 195 points higher at 63523 while the nifty ended at 1885685 up 40 points the gains were led by stocks like hdfc reliance industries and tcs during the intraday trade sensex rose to its fresh record high level of 63588',\n",
       " 'i had a bologna sandwich for lunch it was super delicious i do wish it had cheesey cheese and maybe some mustard mayo relish tomatoes tomato salt pepper butter with a side of hostess cakes')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "token1 = tokenizer.tokenize(basic1, return_str=True)\n",
    "token2 = tokenizer.tokenize(basic2, return_str=True)\n",
    "(token1, token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cabe2b67-2830-462b-975f-7ca3219f6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (article):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a string, article which has been processed with basic_clean\n",
    "    - use nltk.tokenize.ToktokTokenizer to break words and any punctuation left over into discrete units\n",
    "    - returns processed string\n",
    "    \"\"\"\n",
    "    # make the tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    # use tokenizer object to tokenize the article\n",
    "    article = tokenizer.tokenize(article, return_str=True)\n",
    "    # return tokenized article\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77cd48d5-09f3-4cb4-9779-429d27fbb52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benchmark indices sensex and nifty ended at record closing highs on wednesday sensex ended 195 points higher at 63523 while the nifty ended at 1885685 up 40 points the gains were led by stocks like hdfc reliance industries and tcs during the intraday trade sensex rose to its fresh record high level of 63588',\n",
       " 'i had a bologna sandwich for lunch it was super delicious i do wish it had cheesey cheese and maybe some mustard mayo relish tomatoes tomato salt pepper butter with a side of hostess cakes')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = tokenize(basic1)\n",
    "token2 = tokenize(basic2)\n",
    "(token1, token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee9bce63-2722-48b0-a9ee-62dfdbdec8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(article):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a string, article that has been processed with basic_clean and tokenize\n",
    "    - get word stems for each word\n",
    "    - joins the word stems back together in one string and returns that string\n",
    "    \"\"\"\n",
    "    # create the nltk stemmer object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # use ps to get stems of words\n",
    "    stems = [ps.stem(word) for word in article.split(' ')]\n",
    "    # join those words back together and return the string\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a407754-d4d5-46af-841b-27bb6eb2a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the nltk stemmer object\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "stems1 = [ps.stem(word) for word in token1.split(' ')]\n",
    "stems2 = [ps.stem(word) for word in token2.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe4493d4-2902-4f07-9fc6-37164a808c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benchmark indic sensex and nifti end at record close high on wednesday sensex end 195 point higher at 63523 while the nifti end at 1885685 up 40 point the gain were led by stock like hdfc relianc industri and tc dure the intraday trade sensex rose to it fresh record high level of 63588',\n",
       " 'i had a bologna sandwich for lunch it wa super delici i do wish it had cheesey chees and mayb some mustard mayo relish tomato tomato salt pepper butter with a side of hostess cake')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems1 = stem(token1)\n",
    "stems2 = stem(token2)\n",
    "(stems1, stems2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e52425d-193e-44bc-a7cb-1104a6f8b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(article):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a string, article that has been processed with basic_clean and tokenize\n",
    "    - get word roots for each word using nltk.stem.WordNetLemmatizer\n",
    "    - joins the word roots back together in one string and returns that string\n",
    "\"\"\"\n",
    "    # create the wnl lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # use wnl to get stems of words\n",
    "    lemmas = [wnl.lemmatize(word) for word in article.split(' ')]\n",
    "    # join those words back together and return the string\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916350ef-837a-4897-b562-aa133b377c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('benchmark index sensex and nifty ended at record closing high on wednesday sensex ended 195 point higher at 63523 while the nifty ended at 1885685 up 40 point the gain were led by stock like hdfc reliance industry and tc during the intraday trade sensex rose to it fresh record high level of 63588',\n",
       " 'i had a bologna sandwich for lunch it wa super delicious i do wish it had cheesey cheese and maybe some mustard mayo relish tomato tomato salt pepper butter with a side of hostess cake')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas1 = lemmatize(token1)\n",
    "lemmas2 = lemmatize(token2)\n",
    "(lemmas1, lemmas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7931c89d-4be5-4eba-a5cd-3068cec26bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(article, extra_words=[], exclude_words=[]):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a string, article\n",
    "    - accept two optional lists, extra_words and exclude_words\n",
    "    - remove the stopwords from the string (ex: 'a', 'an', 'the', etc.)\n",
    "        - however, it will specifically not remove words that are in exclude_words, even if those words are in the \n",
    "          default stopwords list\n",
    "        - additionally, it will also remove any words in extra_words\n",
    "    - returns the string of non-stopwords\n",
    "    \"\"\"\n",
    "    # get a list of stopwords\n",
    "    stopword_list = stopwords.words('english')\n",
    "    # add in the extra words to remove, use | instead of +\n",
    "    stopword_list = list( set(stopword_list) | set(extra_words) )\n",
    "    # subtract out the words we don't want to remove\n",
    "    stopword_list = list( set(stopword_list) - set(exclude_words) )\n",
    "    # make a list of words in article to iterate on\n",
    "    words = article.split(' ')\n",
    "    # only keep words not in stopword_list\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    # print (f'Removed {len(words) - len(filtered_words)} stopwords.')\n",
    "    # join the filtered words back into one string and return it\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc3efb7-136a-484d-a744-307d5fe836db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 17 stopwords.\n",
      "Removed 14 stopwords.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('benchmark index sensex nifty ended record closing high wednesday sensex ended 195 point higher 63523 nifty ended 1885685 40 point gain led stock like hdfc reliance industry tc intraday trade sensex rose fresh record high level 63588',\n",
       " 'bologna sandwich lunch wa super delicious wish cheesey cheese maybe mustard mayo relish tomato tomato salt pepper butter side hostess cake')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1 = remove_stopwords(lemmas1)\n",
    "final2 = remove_stopwords(lemmas2)\n",
    "(final1, final2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ec93936-80be-47dc-a2d1-18a691cc4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigation below in order to make the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "916da753-5e25-44fb-bf54-9fc2d431a923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get standard stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f4bcb92-231d-4fb0-9b1a-9b9250b13c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fad9b8b3-adfb-4fa9-9bcc-32f52d6b110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some extra words to remove and some exclude_words to leave in\n",
    "extra_words=['tomato', 'salt']\n",
    "exclude_words=['a', 'it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5437f718-7eae-4ec8-b856-523616352b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way to \"subtract\" one list of words from another\n",
    "stopword_list = list (set(stopword_list) - set(exclude_words))\n",
    "\n",
    "# or another way, have to remove words one at a time. ALSO, \n",
    "#  this will throw a fault if the word isn't already in the list you want to remove from\n",
    "#stopword_list.remove(exclude_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ff50427-5ccf-4664-89fc-940b530fc3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(stopword_list)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be5ec3e-81e2-41fb-811f-70fab25ce2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hadn', 'in', 'you', 'will', 'our', 'the', 'had', 'who', 'now', 'can']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to add in words, either append one at a time or do the set thing again; use | instead of \n",
    "stopword_list = list (set(stopword_list) | set(extra_words) )\n",
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15510244-5ceb-4ad9-ab37-81335e2905a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a149f1-1177-43ea-a7c1-02ec6dfc81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = list( set(stopword_list) - set([]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd9a3306-6493-432d-9106-ba3872f1d2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4eab9-4dcf-4218-ba47-d51bb296c45a",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "## 6 acquire a dataframe of the news articles named news_df\n",
    "## 7 acquire a dataframe of the codeup blogs called codeup_df\n",
    "## 8 for each df, produce the following columns\n",
    "    * title\n",
    "    * original\n",
    "    * clean\n",
    "    * stemmed\n",
    "    * lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6dd29e8-f0e8-4464-98b9-84642716c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached file found and read\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Sensex, Nifty end at fresh closing highs</td>\n",
       "      <td>Benchmark indices Sensex and Nifty ended at re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>TIME releases list of the world's 100 most inf...</td>\n",
       "      <td>TIME magazine has released its annual list of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Which are the world's top 10 airlines accordin...</td>\n",
       "      <td>Singapore Airlines is the world's best airline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Loves India, is a fan of PM: Paytm Founder on ...</td>\n",
       "      <td>Paytm Founder Vijay Shekhar Sharma shared a vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>UK's net debt passes 100% of GDP for the first...</td>\n",
       "      <td>The United Kingdom's public sector net debt in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0  business           Sensex, Nifty end at fresh closing highs   \n",
       "1  business  TIME releases list of the world's 100 most inf...   \n",
       "2  business  Which are the world's top 10 airlines accordin...   \n",
       "3  business  Loves India, is a fan of PM: Paytm Founder on ...   \n",
       "4  business  UK's net debt passes 100% of GDP for the first...   \n",
       "\n",
       "                                            original  \n",
       "0  Benchmark indices Sensex and Nifty ended at re...  \n",
       "1  TIME magazine has released its annual list of ...  \n",
       "2  Singapore Airlines is the world's best airline...  \n",
       "3  Paytm Founder Vijay Shekhar Sharma shared a vi...  \n",
       "4  The United Kingdom's public sector net debt in...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the news_df\n",
    "news_list = a.get_news_articles()\n",
    "news_df = pd.DataFrame(news_list)\n",
    "news_df = news_df.rename(columns = {'content':'original'})\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389e7fc-c0bc-44c4-8ec4-f7b969238de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2d3523f-e1d1-4b3e-b838-af7d8134da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached file found and read\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in tech: Panelist Spotlight - Rachel Rob...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Sarah Mell...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Madeleine ...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1  Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2  Women in tech: Panelist Spotlight - Rachel Rob...   \n",
       "3  Women in Tech: Panelist Spotlight - Sarah Mell...   \n",
       "4  Women in Tech: Panelist Spotlight - Madeleine ...   \n",
       "\n",
       "                                            original  \n",
       "0  Spotlight on APIDA Voices: Celebrating Heritag...  \n",
       "1  Women in tech: Panelist Spotlight – Magdalena ...  \n",
       "2  Women in tech: Panelist Spotlight – Rachel Rob...  \n",
       "3  Women in Tech: Panelist Spotlight – Sarah Mell...  \n",
       "4  Women in Tech: Panelist Spotlight – Madeleine ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_list = a.get_all_codeup_blogs()\n",
    "codeup_df = pd.DataFrame(codeup_list)\n",
    "codeup_df = codeup_df.rename(columns = {'content':'original'})\n",
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a09884e2-ac27-43b6-8f40-b4edced0cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_column(df):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a df with at least one column (named 'original') of text to process\n",
    "    - process the text in each row of that column with the basic_clean function\n",
    "    - add a column, named 'clean', to the df with processed text\n",
    "    - return the new df\n",
    "    \"\"\"\n",
    "    df['clean'] = pd.Series([basic_clean(s) for s in df.original])\n",
    "    df['clean'] = pd.Series([tokenize(s) for s in df.clean])\n",
    "    df['clean'] = pd.Series([remove_stopwords(s) for s in df.clean])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da740c-1b7b-441c-b0e2-5b667f2841a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALSO df['clean'] = df.original.apply(basic_clean).apply(tokenize).apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "665d14d3-80db-4888-9b63-49249a81de5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>spotlight apida voices celebrating heritage in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>women tech panelist spotlight magdalena rahn m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in tech: Panelist Spotlight - Rachel Rob...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "      <td>women tech panelist spotlight rachel robbins m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Sarah Mell...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mell...</td>\n",
       "      <td>women tech panelist spotlight sarah mellor mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Madeleine ...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "      <td>women tech panelist spotlight madeleine capper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Why Isn't the San Antonio Tech Scene Growing F...</td>\n",
       "      <td>Why Isn’t the San Antonio Tech Scene Growing F...</td>\n",
       "      <td>isnt san antonio tech scene growing faster aug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Why People Can't Learn Programming on Their Ow...</td>\n",
       "      <td>Why People Can’t Learn Programming on Their Ow...</td>\n",
       "      <td>people cant learn programming aug 14 2018 code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>What is Our Noble Cause? - Codeup</td>\n",
       "      <td>What is Our Noble Cause? Aug 14, 2018 | Codeup...</td>\n",
       "      <td>noble cause aug 14 2018 codeup news tedx san a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Scholarships for Women: Why We're Doing It - C...</td>\n",
       "      <td>Scholarships for Women: Why We’re Doing It Aug...</td>\n",
       "      <td>scholarships women aug 14 2018 codeup news hot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Student Profile: Leslie Tolbert - Codeup</td>\n",
       "      <td>Student Profile: Leslie Tolbert Aug 14, 2018 |...</td>\n",
       "      <td>student profile leslie tolbert aug 14 2018 alu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1    Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2    Women in tech: Panelist Spotlight - Rachel Rob...   \n",
       "3    Women in Tech: Panelist Spotlight - Sarah Mell...   \n",
       "4    Women in Tech: Panelist Spotlight - Madeleine ...   \n",
       "..                                                 ...   \n",
       "265  Why Isn't the San Antonio Tech Scene Growing F...   \n",
       "266  Why People Can't Learn Programming on Their Ow...   \n",
       "267                  What is Our Noble Cause? - Codeup   \n",
       "268  Scholarships for Women: Why We're Doing It - C...   \n",
       "269           Student Profile: Leslie Tolbert - Codeup   \n",
       "\n",
       "                                              original  \\\n",
       "0    Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1    Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2    Women in tech: Panelist Spotlight – Rachel Rob...   \n",
       "3    Women in Tech: Panelist Spotlight – Sarah Mell...   \n",
       "4    Women in Tech: Panelist Spotlight – Madeleine ...   \n",
       "..                                                 ...   \n",
       "265  Why Isn’t the San Antonio Tech Scene Growing F...   \n",
       "266  Why People Can’t Learn Programming on Their Ow...   \n",
       "267  What is Our Noble Cause? Aug 14, 2018 | Codeup...   \n",
       "268  Scholarships for Women: Why We’re Doing It Aug...   \n",
       "269  Student Profile: Leslie Tolbert Aug 14, 2018 |...   \n",
       "\n",
       "                                                 clean  \n",
       "0    spotlight apida voices celebrating heritage in...  \n",
       "1    women tech panelist spotlight magdalena rahn m...  \n",
       "2    women tech panelist spotlight rachel robbins m...  \n",
       "3    women tech panelist spotlight sarah mellor mar...  \n",
       "4    women tech panelist spotlight madeleine capper...  \n",
       "..                                                 ...  \n",
       "265  isnt san antonio tech scene growing faster aug...  \n",
       "266  people cant learn programming aug 14 2018 code...  \n",
       "267  noble cause aug 14 2018 codeup news tedx san a...  \n",
       "268  scholarships women aug 14 2018 codeup news hot...  \n",
       "269  student profile leslie tolbert aug 14 2018 alu...  \n",
       "\n",
       "[270 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = get_clean_column(codeup_df)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2c7136d-98e8-4a47-b05e-d54076774fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_column(df):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a df with at least one column (named 'clean') of text to process\n",
    "    - process the text in each row of that column with the stem function\n",
    "    - add a column named 'stemmed' to the df with processed text\n",
    "    - return the new df\n",
    "    \"\"\"\n",
    "    df['stemmed'] = pd.Series([stem(s) for s in df.clean])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bba9a0b4-b8a6-415b-aa63-59baca12b0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>spotlight on apida voices celebrating heritage...</td>\n",
       "      <td>spotlight on apida voic celebr heritag and ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>women in tech panelist spotlight  magdalena ra...</td>\n",
       "      <td>women in tech panelist spotlight  magdalena ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in tech: Panelist Spotlight - Rachel Rob...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "      <td>women in tech panelist spotlight  rachel robbi...</td>\n",
       "      <td>women in tech panelist spotlight  rachel robbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Sarah Mell...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mell...</td>\n",
       "      <td>women in tech panelist spotlight  sarah mellor...</td>\n",
       "      <td>women in tech panelist spotlight  sarah mellor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Madeleine ...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "      <td>women in tech panelist spotlight  madeleine ca...</td>\n",
       "      <td>women in tech panelist spotlight  madelein cap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Why Isn't the San Antonio Tech Scene Growing F...</td>\n",
       "      <td>Why Isn’t the San Antonio Tech Scene Growing F...</td>\n",
       "      <td>why isnt the san antonio tech scene growing fa...</td>\n",
       "      <td>whi isnt the san antonio tech scene grow faste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Why People Can't Learn Programming on Their Ow...</td>\n",
       "      <td>Why People Can’t Learn Programming on Their Ow...</td>\n",
       "      <td>why people cant learn programming on their own...</td>\n",
       "      <td>whi peopl cant learn program on their own aug ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>What is Our Noble Cause? - Codeup</td>\n",
       "      <td>What is Our Noble Cause? Aug 14, 2018 | Codeup...</td>\n",
       "      <td>what is our noble cause aug 14 2018  codeup ne...</td>\n",
       "      <td>what is our nobl caus aug 14 2018  codeup news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Scholarships for Women: Why We're Doing It - C...</td>\n",
       "      <td>Scholarships for Women: Why We’re Doing It Aug...</td>\n",
       "      <td>scholarships for women why were doing it aug 1...</td>\n",
       "      <td>scholarship for women whi were do it aug 14 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Student Profile: Leslie Tolbert - Codeup</td>\n",
       "      <td>Student Profile: Leslie Tolbert Aug 14, 2018 |...</td>\n",
       "      <td>student profile leslie tolbert aug 14 2018  al...</td>\n",
       "      <td>student profil lesli tolbert aug 14 2018  alum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1    Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2    Women in tech: Panelist Spotlight - Rachel Rob...   \n",
       "3    Women in Tech: Panelist Spotlight - Sarah Mell...   \n",
       "4    Women in Tech: Panelist Spotlight - Madeleine ...   \n",
       "..                                                 ...   \n",
       "265  Why Isn't the San Antonio Tech Scene Growing F...   \n",
       "266  Why People Can't Learn Programming on Their Ow...   \n",
       "267                  What is Our Noble Cause? - Codeup   \n",
       "268  Scholarships for Women: Why We're Doing It - C...   \n",
       "269           Student Profile: Leslie Tolbert - Codeup   \n",
       "\n",
       "                                              original  \\\n",
       "0    Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1    Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2    Women in tech: Panelist Spotlight – Rachel Rob...   \n",
       "3    Women in Tech: Panelist Spotlight – Sarah Mell...   \n",
       "4    Women in Tech: Panelist Spotlight – Madeleine ...   \n",
       "..                                                 ...   \n",
       "265  Why Isn’t the San Antonio Tech Scene Growing F...   \n",
       "266  Why People Can’t Learn Programming on Their Ow...   \n",
       "267  What is Our Noble Cause? Aug 14, 2018 | Codeup...   \n",
       "268  Scholarships for Women: Why We’re Doing It Aug...   \n",
       "269  Student Profile: Leslie Tolbert Aug 14, 2018 |...   \n",
       "\n",
       "                                                 clean  \\\n",
       "0    spotlight on apida voices celebrating heritage...   \n",
       "1    women in tech panelist spotlight  magdalena ra...   \n",
       "2    women in tech panelist spotlight  rachel robbi...   \n",
       "3    women in tech panelist spotlight  sarah mellor...   \n",
       "4    women in tech panelist spotlight  madeleine ca...   \n",
       "..                                                 ...   \n",
       "265  why isnt the san antonio tech scene growing fa...   \n",
       "266  why people cant learn programming on their own...   \n",
       "267  what is our noble cause aug 14 2018  codeup ne...   \n",
       "268  scholarships for women why were doing it aug 1...   \n",
       "269  student profile leslie tolbert aug 14 2018  al...   \n",
       "\n",
       "                                               stemmed  \n",
       "0    spotlight on apida voic celebr heritag and ins...  \n",
       "1    women in tech panelist spotlight  magdalena ra...  \n",
       "2    women in tech panelist spotlight  rachel robbi...  \n",
       "3    women in tech panelist spotlight  sarah mellor...  \n",
       "4    women in tech panelist spotlight  madelein cap...  \n",
       "..                                                 ...  \n",
       "265  whi isnt the san antonio tech scene grow faste...  \n",
       "266  whi peopl cant learn program on their own aug ...  \n",
       "267  what is our nobl caus aug 14 2018  codeup news...  \n",
       "268  scholarship for women whi were do it aug 14 20...  \n",
       "269  student profil lesli tolbert aug 14 2018  alum...  \n",
       "\n",
       "[270 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = p.get_stemmed_column(codeup_df)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d746c4c-98f5-4eef-923e-d8fa71b6adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatized_column(df):\n",
    "    \"\"\"\n",
    "    This function will\n",
    "    - accept a df with at least one column (named 'clean') of text to process\n",
    "    - process the text in each row of that column with the lemmatize function\n",
    "    - add a column named 'lemmatized' to the df with processed text\n",
    "    - return the new df\n",
    "    \"\"\"\n",
    "    df['lemmatized'] = pd.Series([lemmatize(s) for s in df.clean])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e8a9464-5f34-4112-b573-be2bb69a9281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>Spotlight on APIDA Voices: Celebrating Heritag...</td>\n",
       "      <td>spotlight on apida voices celebrating heritage...</td>\n",
       "      <td>spotlight on apida voic celebr heritag and ins...</td>\n",
       "      <td>spotlight on apida voice celebrating heritage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Magdalena ...</td>\n",
       "      <td>women in tech panelist spotlight  magdalena ra...</td>\n",
       "      <td>women in tech panelist spotlight  magdalena ra...</td>\n",
       "      <td>woman in tech panelist spotlight  magdalena ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Women in tech: Panelist Spotlight - Rachel Rob...</td>\n",
       "      <td>Women in tech: Panelist Spotlight – Rachel Rob...</td>\n",
       "      <td>women in tech panelist spotlight  rachel robbi...</td>\n",
       "      <td>women in tech panelist spotlight  rachel robbi...</td>\n",
       "      <td>woman in tech panelist spotlight  rachel robbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Sarah Mell...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Sarah Mell...</td>\n",
       "      <td>women in tech panelist spotlight  sarah mellor...</td>\n",
       "      <td>women in tech panelist spotlight  sarah mellor...</td>\n",
       "      <td>woman in tech panelist spotlight  sarah mellor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women in Tech: Panelist Spotlight - Madeleine ...</td>\n",
       "      <td>Women in Tech: Panelist Spotlight – Madeleine ...</td>\n",
       "      <td>women in tech panelist spotlight  madeleine ca...</td>\n",
       "      <td>women in tech panelist spotlight  madelein cap...</td>\n",
       "      <td>woman in tech panelist spotlight  madeleine ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Why Isn't the San Antonio Tech Scene Growing F...</td>\n",
       "      <td>Why Isn’t the San Antonio Tech Scene Growing F...</td>\n",
       "      <td>why isnt the san antonio tech scene growing fa...</td>\n",
       "      <td>whi isnt the san antonio tech scene grow faste...</td>\n",
       "      <td>why isnt the san antonio tech scene growing fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Why People Can't Learn Programming on Their Ow...</td>\n",
       "      <td>Why People Can’t Learn Programming on Their Ow...</td>\n",
       "      <td>why people cant learn programming on their own...</td>\n",
       "      <td>whi peopl cant learn program on their own aug ...</td>\n",
       "      <td>why people cant learn programming on their own...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>What is Our Noble Cause? - Codeup</td>\n",
       "      <td>What is Our Noble Cause? Aug 14, 2018 | Codeup...</td>\n",
       "      <td>what is our noble cause aug 14 2018  codeup ne...</td>\n",
       "      <td>what is our nobl caus aug 14 2018  codeup news...</td>\n",
       "      <td>what is our noble cause aug 14 2018  codeup ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Scholarships for Women: Why We're Doing It - C...</td>\n",
       "      <td>Scholarships for Women: Why We’re Doing It Aug...</td>\n",
       "      <td>scholarships for women why were doing it aug 1...</td>\n",
       "      <td>scholarship for women whi were do it aug 14 20...</td>\n",
       "      <td>scholarship for woman why were doing it aug 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Student Profile: Leslie Tolbert - Codeup</td>\n",
       "      <td>Student Profile: Leslie Tolbert Aug 14, 2018 |...</td>\n",
       "      <td>student profile leslie tolbert aug 14 2018  al...</td>\n",
       "      <td>student profil lesli tolbert aug 14 2018  alum...</td>\n",
       "      <td>student profile leslie tolbert aug 14 2018  al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1    Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2    Women in tech: Panelist Spotlight - Rachel Rob...   \n",
       "3    Women in Tech: Panelist Spotlight - Sarah Mell...   \n",
       "4    Women in Tech: Panelist Spotlight - Madeleine ...   \n",
       "..                                                 ...   \n",
       "265  Why Isn't the San Antonio Tech Scene Growing F...   \n",
       "266  Why People Can't Learn Programming on Their Ow...   \n",
       "267                  What is Our Noble Cause? - Codeup   \n",
       "268  Scholarships for Women: Why We're Doing It - C...   \n",
       "269           Student Profile: Leslie Tolbert - Codeup   \n",
       "\n",
       "                                              original  \\\n",
       "0    Spotlight on APIDA Voices: Celebrating Heritag...   \n",
       "1    Women in tech: Panelist Spotlight – Magdalena ...   \n",
       "2    Women in tech: Panelist Spotlight – Rachel Rob...   \n",
       "3    Women in Tech: Panelist Spotlight – Sarah Mell...   \n",
       "4    Women in Tech: Panelist Spotlight – Madeleine ...   \n",
       "..                                                 ...   \n",
       "265  Why Isn’t the San Antonio Tech Scene Growing F...   \n",
       "266  Why People Can’t Learn Programming on Their Ow...   \n",
       "267  What is Our Noble Cause? Aug 14, 2018 | Codeup...   \n",
       "268  Scholarships for Women: Why We’re Doing It Aug...   \n",
       "269  Student Profile: Leslie Tolbert Aug 14, 2018 |...   \n",
       "\n",
       "                                                 clean  \\\n",
       "0    spotlight on apida voices celebrating heritage...   \n",
       "1    women in tech panelist spotlight  magdalena ra...   \n",
       "2    women in tech panelist spotlight  rachel robbi...   \n",
       "3    women in tech panelist spotlight  sarah mellor...   \n",
       "4    women in tech panelist spotlight  madeleine ca...   \n",
       "..                                                 ...   \n",
       "265  why isnt the san antonio tech scene growing fa...   \n",
       "266  why people cant learn programming on their own...   \n",
       "267  what is our noble cause aug 14 2018  codeup ne...   \n",
       "268  scholarships for women why were doing it aug 1...   \n",
       "269  student profile leslie tolbert aug 14 2018  al...   \n",
       "\n",
       "                                               stemmed  \\\n",
       "0    spotlight on apida voic celebr heritag and ins...   \n",
       "1    women in tech panelist spotlight  magdalena ra...   \n",
       "2    women in tech panelist spotlight  rachel robbi...   \n",
       "3    women in tech panelist spotlight  sarah mellor...   \n",
       "4    women in tech panelist spotlight  madelein cap...   \n",
       "..                                                 ...   \n",
       "265  whi isnt the san antonio tech scene grow faste...   \n",
       "266  whi peopl cant learn program on their own aug ...   \n",
       "267  what is our nobl caus aug 14 2018  codeup news...   \n",
       "268  scholarship for women whi were do it aug 14 20...   \n",
       "269  student profil lesli tolbert aug 14 2018  alum...   \n",
       "\n",
       "                                            lemmatized  \n",
       "0    spotlight on apida voice celebrating heritage ...  \n",
       "1    woman in tech panelist spotlight  magdalena ra...  \n",
       "2    woman in tech panelist spotlight  rachel robbi...  \n",
       "3    woman in tech panelist spotlight  sarah mellor...  \n",
       "4    woman in tech panelist spotlight  madeleine ca...  \n",
       "..                                                 ...  \n",
       "265  why isnt the san antonio tech scene growing fa...  \n",
       "266  why people cant learn programming on their own...  \n",
       "267  what is our noble cause aug 14 2018  codeup ne...  \n",
       "268  scholarship for woman why were doing it aug 14...  \n",
       "269  student profile leslie tolbert aug 14 2018  al...  \n",
       "\n",
       "[270 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeup_df = p.get_lemmatized_column(codeup_df)\n",
    "codeup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb965b5c-ea14-48cf-844d-6ed78661558a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Sensex, Nifty end at fresh closing highs</td>\n",
       "      <td>Benchmark indices Sensex and Nifty ended at re...</td>\n",
       "      <td>benchmark indices sensex and nifty ended at re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>TIME releases list of the world's 100 most inf...</td>\n",
       "      <td>TIME magazine has released its annual list of ...</td>\n",
       "      <td>time magazine has released its annual list of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Which are the world's top 10 airlines accordin...</td>\n",
       "      <td>Singapore Airlines is the world's best airline...</td>\n",
       "      <td>singapore airlines is the world's best airline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Loves India, is a fan of PM: Paytm Founder on ...</td>\n",
       "      <td>Paytm Founder Vijay Shekhar Sharma shared a vi...</td>\n",
       "      <td>paytm founder vijay shekhar sharma shared a vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>UK's net debt passes 100% of GDP for the first...</td>\n",
       "      <td>The United Kingdom's public sector net debt in...</td>\n",
       "      <td>the united kingdom's public sector net debt in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0  business           Sensex, Nifty end at fresh closing highs   \n",
       "1  business  TIME releases list of the world's 100 most inf...   \n",
       "2  business  Which are the world's top 10 airlines accordin...   \n",
       "3  business  Loves India, is a fan of PM: Paytm Founder on ...   \n",
       "4  business  UK's net debt passes 100% of GDP for the first...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Benchmark indices Sensex and Nifty ended at re...   \n",
       "1  TIME magazine has released its annual list of ...   \n",
       "2  Singapore Airlines is the world's best airline...   \n",
       "3  Paytm Founder Vijay Shekhar Sharma shared a vi...   \n",
       "4  The United Kingdom's public sector net debt in...   \n",
       "\n",
       "                                               clean  \n",
       "0  benchmark indices sensex and nifty ended at re...  \n",
       "1  time magazine has released its annual list of ...  \n",
       "2  singapore airlines is the world's best airline...  \n",
       "3  paytm founder vijay shekhar sharma shared a vi...  \n",
       "4  the united kingdom's public sector net debt in...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = get_clean_column(news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdf6d95e-4a21-4fec-b5e5-306fe9210afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Sensex, Nifty end at fresh closing highs</td>\n",
       "      <td>Benchmark indices Sensex and Nifty ended at re...</td>\n",
       "      <td>benchmark indices sensex and nifty ended at re...</td>\n",
       "      <td>benchmark indic sensex and nifti end at record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>TIME releases list of the world's 100 most inf...</td>\n",
       "      <td>TIME magazine has released its annual list of ...</td>\n",
       "      <td>time magazine has released its annual list of ...</td>\n",
       "      <td>time magazin ha releas it annual list of the w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Which are the world's top 10 airlines accordin...</td>\n",
       "      <td>Singapore Airlines is the world's best airline...</td>\n",
       "      <td>singapore airlines is the world's best airline...</td>\n",
       "      <td>singapor airlin is the world' best airlin acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Loves India, is a fan of PM: Paytm Founder on ...</td>\n",
       "      <td>Paytm Founder Vijay Shekhar Sharma shared a vi...</td>\n",
       "      <td>paytm founder vijay shekhar sharma shared a vi...</td>\n",
       "      <td>paytm founder vijay shekhar sharma share a vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>UK's net debt passes 100% of GDP for the first...</td>\n",
       "      <td>The United Kingdom's public sector net debt in...</td>\n",
       "      <td>the united kingdom's public sector net debt in...</td>\n",
       "      <td>the unit kingdom' public sector net debt in ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0  business           Sensex, Nifty end at fresh closing highs   \n",
       "1  business  TIME releases list of the world's 100 most inf...   \n",
       "2  business  Which are the world's top 10 airlines accordin...   \n",
       "3  business  Loves India, is a fan of PM: Paytm Founder on ...   \n",
       "4  business  UK's net debt passes 100% of GDP for the first...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Benchmark indices Sensex and Nifty ended at re...   \n",
       "1  TIME magazine has released its annual list of ...   \n",
       "2  Singapore Airlines is the world's best airline...   \n",
       "3  Paytm Founder Vijay Shekhar Sharma shared a vi...   \n",
       "4  The United Kingdom's public sector net debt in...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  benchmark indices sensex and nifty ended at re...   \n",
       "1  time magazine has released its annual list of ...   \n",
       "2  singapore airlines is the world's best airline...   \n",
       "3  paytm founder vijay shekhar sharma shared a vi...   \n",
       "4  the united kingdom's public sector net debt in...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  benchmark indic sensex and nifti end at record...  \n",
       "1  time magazin ha releas it annual list of the w...  \n",
       "2  singapor airlin is the world' best airlin acco...  \n",
       "3  paytm founder vijay shekhar sharma share a vid...  \n",
       "4  the unit kingdom' public sector net debt in ma...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = get_stemmed_column(news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5a22340-b168-410d-a275-9e183d2dc527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>original</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Sensex, Nifty end at fresh closing highs</td>\n",
       "      <td>Benchmark indices Sensex and Nifty ended at re...</td>\n",
       "      <td>benchmark indices sensex and nifty ended at re...</td>\n",
       "      <td>benchmark indic sensex and nifti end at record...</td>\n",
       "      <td>benchmark index sensex and nifty ended at reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>TIME releases list of the world's 100 most inf...</td>\n",
       "      <td>TIME magazine has released its annual list of ...</td>\n",
       "      <td>time magazine has released its annual list of ...</td>\n",
       "      <td>time magazin ha releas it annual list of the w...</td>\n",
       "      <td>time magazine ha released it annual list of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Which are the world's top 10 airlines accordin...</td>\n",
       "      <td>Singapore Airlines is the world's best airline...</td>\n",
       "      <td>singapore airlines is the world's best airline...</td>\n",
       "      <td>singapor airlin is the world' best airlin acco...</td>\n",
       "      <td>singapore airline is the world's best airline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Loves India, is a fan of PM: Paytm Founder on ...</td>\n",
       "      <td>Paytm Founder Vijay Shekhar Sharma shared a vi...</td>\n",
       "      <td>paytm founder vijay shekhar sharma shared a vi...</td>\n",
       "      <td>paytm founder vijay shekhar sharma share a vid...</td>\n",
       "      <td>paytm founder vijay shekhar sharma shared a vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>UK's net debt passes 100% of GDP for the first...</td>\n",
       "      <td>The United Kingdom's public sector net debt in...</td>\n",
       "      <td>the united kingdom's public sector net debt in...</td>\n",
       "      <td>the unit kingdom' public sector net debt in ma...</td>\n",
       "      <td>the united kingdom's public sector net debt in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0  business           Sensex, Nifty end at fresh closing highs   \n",
       "1  business  TIME releases list of the world's 100 most inf...   \n",
       "2  business  Which are the world's top 10 airlines accordin...   \n",
       "3  business  Loves India, is a fan of PM: Paytm Founder on ...   \n",
       "4  business  UK's net debt passes 100% of GDP for the first...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Benchmark indices Sensex and Nifty ended at re...   \n",
       "1  TIME magazine has released its annual list of ...   \n",
       "2  Singapore Airlines is the world's best airline...   \n",
       "3  Paytm Founder Vijay Shekhar Sharma shared a vi...   \n",
       "4  The United Kingdom's public sector net debt in...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  benchmark indices sensex and nifty ended at re...   \n",
       "1  time magazine has released its annual list of ...   \n",
       "2  singapore airlines is the world's best airline...   \n",
       "3  paytm founder vijay shekhar sharma shared a vi...   \n",
       "4  the united kingdom's public sector net debt in...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  benchmark indic sensex and nifti end at record...   \n",
       "1  time magazin ha releas it annual list of the w...   \n",
       "2  singapor airlin is the world' best airlin acco...   \n",
       "3  paytm founder vijay shekhar sharma share a vid...   \n",
       "4  the unit kingdom' public sector net debt in ma...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  benchmark index sensex and nifty ended at reco...  \n",
       "1  time magazine ha released it annual list of th...  \n",
       "2  singapore airline is the world's best airline ...  \n",
       "3  paytm founder vijay shekhar sharma shared a vi...  \n",
       "4  the united kingdom's public sector net debt in...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = get_lemmatized_column(news_df)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6458f-ab05-46b3-bd8f-01c62d0121f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "442a3ced-9c1c-421c-9063-fa936f8a4681",
   "metadata": {},
   "source": [
    "# NEED TO MAKE A FUNCTION THAT DOES ALL OF THE ABOVE, clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70f8a9-70ad-451a-b7d2-1a433d7b632e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ecb4ba8-25cf-47f9-8f0d-6356b2a82a05",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "## 9. Ask yourself:\n",
    "\n",
    "    * If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "        * Probably doesn't matter\n",
    "    * If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "        * Still probably doesn't matter, but I know stemmed will run faster\n",
    "    * If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "        * lemmatized word strings were consistently longer than stemmed word strings, so stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6e99e8d-4eca-4ee7-b9b2-95573a031183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem_len: 287, lemma_len: 298\n",
      "stem_len: 339, lemma_len: 369\n",
      "stem_len: 319, lemma_len: 343\n",
      "stem_len: 311, lemma_len: 329\n",
      "stem_len: 329, lemma_len: 359\n",
      "stem_len: 333, lemma_len: 372\n",
      "stem_len: 287, lemma_len: 305\n",
      "stem_len: 310, lemma_len: 336\n",
      "stem_len: 321, lemma_len: 350\n",
      "stem_len: 309, lemma_len: 341\n",
      "stem_len: 309, lemma_len: 337\n",
      "stem_len: 332, lemma_len: 380\n",
      "stem_len: 338, lemma_len: 364\n",
      "stem_len: 345, lemma_len: 367\n",
      "stem_len: 313, lemma_len: 338\n",
      "stem_len: 320, lemma_len: 339\n",
      "stem_len: 339, lemma_len: 361\n",
      "stem_len: 297, lemma_len: 316\n",
      "stem_len: 318, lemma_len: 362\n",
      "stem_len: 339, lemma_len: 372\n",
      "stem_len: 284, lemma_len: 308\n",
      "stem_len: 313, lemma_len: 358\n",
      "stem_len: 324, lemma_len: 361\n",
      "stem_len: 319, lemma_len: 359\n",
      "stem_len: 308, lemma_len: 350\n",
      "stem_len: 325, lemma_len: 352\n",
      "stem_len: 256, lemma_len: 290\n",
      "stem_len: 320, lemma_len: 337\n",
      "stem_len: 353, lemma_len: 368\n",
      "stem_len: 326, lemma_len: 363\n",
      "stem_len: 322, lemma_len: 345\n",
      "stem_len: 330, lemma_len: 342\n",
      "stem_len: 299, lemma_len: 309\n",
      "stem_len: 313, lemma_len: 340\n",
      "stem_len: 341, lemma_len: 372\n",
      "stem_len: 306, lemma_len: 319\n",
      "stem_len: 319, lemma_len: 344\n",
      "stem_len: 349, lemma_len: 375\n",
      "stem_len: 314, lemma_len: 331\n",
      "stem_len: 338, lemma_len: 349\n",
      "stem_len: 346, lemma_len: 368\n",
      "stem_len: 323, lemma_len: 341\n",
      "stem_len: 346, lemma_len: 367\n",
      "stem_len: 343, lemma_len: 370\n",
      "stem_len: 314, lemma_len: 333\n",
      "stem_len: 337, lemma_len: 351\n",
      "stem_len: 337, lemma_len: 349\n",
      "stem_len: 335, lemma_len: 359\n",
      "stem_len: 308, lemma_len: 325\n",
      "stem_len: 358, lemma_len: 380\n",
      "stem_len: 278, lemma_len: 294\n",
      "stem_len: 295, lemma_len: 330\n",
      "stem_len: 294, lemma_len: 324\n",
      "stem_len: 328, lemma_len: 368\n",
      "stem_len: 310, lemma_len: 343\n",
      "stem_len: 339, lemma_len: 369\n",
      "stem_len: 322, lemma_len: 351\n",
      "stem_len: 328, lemma_len: 353\n",
      "stem_len: 344, lemma_len: 371\n",
      "stem_len: 336, lemma_len: 375\n",
      "stem_len: 309, lemma_len: 321\n",
      "stem_len: 314, lemma_len: 340\n",
      "stem_len: 346, lemma_len: 377\n",
      "stem_len: 313, lemma_len: 354\n",
      "stem_len: 338, lemma_len: 386\n",
      "stem_len: 337, lemma_len: 385\n",
      "stem_len: 345, lemma_len: 380\n",
      "stem_len: 343, lemma_len: 379\n",
      "stem_len: 301, lemma_len: 321\n",
      "stem_len: 274, lemma_len: 306\n",
      "stem_len: 324, lemma_len: 353\n",
      "stem_len: 325, lemma_len: 366\n",
      "stem_len: 278, lemma_len: 310\n",
      "stem_len: 340, lemma_len: 370\n",
      "stem_len: 320, lemma_len: 339\n",
      "stem_len: 268, lemma_len: 280\n",
      "stem_len: 310, lemma_len: 339\n",
      "stem_len: 255, lemma_len: 272\n",
      "stem_len: 321, lemma_len: 342\n",
      "stem_len: 333, lemma_len: 361\n",
      "stem_len: 280, lemma_len: 292\n",
      "stem_len: 280, lemma_len: 300\n",
      "stem_len: 301, lemma_len: 332\n",
      "stem_len: 262, lemma_len: 278\n",
      "stem_len: 296, lemma_len: 335\n",
      "stem_len: 323, lemma_len: 341\n",
      "stem_len: 314, lemma_len: 339\n",
      "stem_len: 291, lemma_len: 316\n",
      "stem_len: 311, lemma_len: 331\n",
      "stem_len: 261, lemma_len: 285\n",
      "stem_len: 274, lemma_len: 303\n",
      "stem_len: 303, lemma_len: 321\n",
      "stem_len: 302, lemma_len: 332\n",
      "stem_len: 331, lemma_len: 364\n",
      "stem_len: 321, lemma_len: 344\n",
      "stem_len: 335, lemma_len: 368\n",
      "stem_len: 269, lemma_len: 287\n",
      "stem_len: 332, lemma_len: 349\n",
      "stem_len: 303, lemma_len: 325\n",
      "stem_len: 311, lemma_len: 341\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, len(news_df)):\n",
    "    print(f'stem_len: {len(news_df.stemmed[i])}, lemma_len: {len(news_df.lemmatized[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00f637-e730-49f3-931b-eae4047c9b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb49bd8-1fdb-4473-8550-4898f8230da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f064a-eaad-43d6-922c-2254bcc5baea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe375d-349d-43ab-97f8-fac76998cc22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
